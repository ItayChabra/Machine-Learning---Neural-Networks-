{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Itay Chabra  \n",
        "Neta Ben Mordechai"
      ],
      "metadata": {
        "id": "EzeOmDgpjB21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "6SKiRzPuiHZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGE_bWIK10sZ",
        "outputId": "a5f9a4dc-2e3e-4892-a9c6-2b4ff5d44a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.1.24)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras-preprocessing-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for keras-preprocessing-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n"
          ]
        }
      ],
      "source": [
        "#!pip install language_models\n",
        "!pip install tensorflow-gpu\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install keras-preprocessing-gpu\n",
        "import tensorflow as tf\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO6vHdEG29zP",
        "outputId": "abb0a1ca-7a95-4167-dab7-e7224e835beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/device:GPU:0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/netabm/language_models.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mobnO9jk3AIy",
        "outputId": "7d2cb892-cf66-40bf-c67b-e7de1ec4f1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'language_models'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 109 (delta 57), reused 105 (delta 55), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (109/109), 19.38 MiB | 21.76 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.datasets import imdb\n",
        "from keras.layers import  LSTM, Embedding, TimeDistributed, Input, Dense\n",
        "from keras.models import Model\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os, random\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from language_models import util\n",
        "\n",
        "CHECK = 5"
      ],
      "metadata": {
        "id": "FGRqjrtk3u6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model : Model, seed, size, temperature=1.0):\n",
        "    \"\"\"\n",
        "    :param model: The complete RNN language model\n",
        "    :param seed: The first few wordas of the sequence to start generating from\n",
        "    :param size: The total size of the sequence to generate\n",
        "    :param temperature: This controls how much we follow the probabilities provided by the network. For t=1.0 we just\n",
        "        sample directly according to the probabilities. Lower temperatures make the high-probability words more likely\n",
        "        (providing more likely, but slightly boring sentences) and higher temperatures make the lower probabilities more\n",
        "        likely (resulting is weirder sentences). For temperature=0.0, the generation is _greedy_, i.e. the word with the\n",
        "        highest probability is always chosen.\n",
        "    :return: A list of integers representing a samples sentence\n",
        "    \"\"\"\n",
        "\n",
        "    ls = seed.shape[0]\n",
        "\n",
        "    # Due to the way Keras RNNs work, we feed the model a complete sequence each time. At first it's just the seed,\n",
        "    # zero-padded to the right length. With each iteration we sample and set the next character.\n",
        "\n",
        "    tokens = np.concatenate([seed, np.zeros(size - ls)])\n",
        "\n",
        "    for i in range(ls, size):\n",
        "\n",
        "        probs = model.predict(tokens[None,:])\n",
        "\n",
        "        # Extract the i-th probability vector and sample an index from it\n",
        "        next_token = util.sample_logits(probs[0, i-1, :], temperature=temperature)\n",
        "\n",
        "        tokens[i] = next_token\n",
        "\n",
        "    return [int(t) for t in tokens]"
      ],
      "metadata": {
        "id": "TCiR0ckm5bxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_loss(y_true, y_pred):\n",
        "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "metadata": {
        "id": "cVhiXYR87qeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  epochs = 20 # Number of epochs\n",
        "  embedding_size = 300 # Size of the word embeddings on the input layer.\n",
        "  out_every = 1 # Output every n epochs.\n",
        "  lr = 0.001 # Learning rate\n",
        "  batch = 128 # Batch size\n",
        "  task = 'wikisimple'\n",
        "  data = './data' # Data file. Should contain one sentence per line.\n",
        "  lstm_capacity = 256\n",
        "  max_length = None # Sentence max length.\n",
        "  top_words = 10000 # Word list size.\n",
        "  limit = None # Character cap for the corpus - not relevant in our exercise.\n",
        "  tb_dir = './runs/words' # Tensorboard directory\n",
        "  seed = -1 # RNG seed. Negative for random (seed is printed for reproducability).\n",
        "  extra = None # Number of extra LSTM layers.\n",
        "\n",
        "options = Args()"
      ],
      "metadata": {
        "id": "uClBMJYJ7tCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbw = SummaryWriter(log_dir=options.tb_dir)\n",
        "\n",
        "if options.seed < 0:\n",
        "    seed = random.randint(0, 1000000)\n",
        "    print('random seed: ', seed)\n",
        "    np.random.seed(seed)\n",
        "else:\n",
        "    np.random.seed(options.seed)\n",
        "\n",
        "if options.task == 'wikisimple':\n",
        "\n",
        "    x, w2i, i2w = util.load_words(util.DIR + '/datasets/wikisimple.txt', vocab_size=options.top_words, limit=options.limit)\n",
        "\n",
        "    # Finding the length of the longest sequence\n",
        "    x_max_len = max([len(sentence) for sentence in x])\n",
        "\n",
        "    numwords = len(i2w)\n",
        "    print('max sequence length ', x_max_len)\n",
        "    print(numwords, 'distinct words')\n",
        "\n",
        "    x = util.batch_pad(x, options.batch, add_eos=True)\n",
        "\n",
        "elif options.task == 'file':\n",
        "\n",
        "    x, w2i, i2w = util.load_words(options.data_dir, vocab_size=options.top_words, limit=options.limit)\n",
        "\n",
        "    # Finding the length of the longest sequence\n",
        "    x_max_len = max([len(sentence) for sentence in x])\n",
        "\n",
        "    numwords = len(i2w)\n",
        "    print('max sequence length ', x_max_len)\n",
        "    print(numwords, 'distinct words')\n",
        "\n",
        "    x = util.batch_pad(x, options.batch, add_eos=True)\n",
        "\n",
        "else:\n",
        "    raise Exception('Task {} not recognized.'.format(options.task))\n",
        "\n",
        "def decode(seq):\n",
        "    return ' '.join(i2w[id] for id in seq)\n",
        "\n",
        "print('Finished data loading. ', sum([b.shape[0] for b in x]), ' sentences loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2o0hR_p7wz7",
        "outputId": "bafeba70-803f-4ede-af2a-d97aa5863157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random seed:  959238\n",
            "raw data read\n",
            "max sequence length  132\n",
            "10000 distinct words\n",
            "max length per batch:  [15, 15, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 34, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 37, 37, 38, 38, 39, 39, 40, 40, 41, 42, 42, 43, 44, 45, 46, 47, 48, 50, 52, 55, 60, 75, 133]\n",
            "Finished data loading.  29741  sentences loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Validation Test Split"
      ],
      "metadata": {
        "id": "iNg8QlpRiOmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: List of sequences or sentences\n",
        "    - train_ratio: The proportion of data for the training set (default 0.8)\n",
        "    - val_ratio: The proportion of data for the validation set (default 0.1)\n",
        "    - test_ratio: The proportion of data for the test set (default 0.1)\n",
        "\n",
        "    Returns:\n",
        "    - train_data, val_data, test_data: The split datasets\n",
        "    \"\"\"\n",
        "    # Check that the ratios sum to 1\n",
        "    if train_ratio + val_ratio + test_ratio != 1.0:\n",
        "        raise ValueError(\"The sum of train, validation, and test ratios must equal 1.\")\n",
        "\n",
        "    # Get the total number of samples in the dataset\n",
        "    total_len = len(dataset)\n",
        "\n",
        "    # Calculate the split lengths\n",
        "    train_len = int(total_len * train_ratio)\n",
        "    val_len = int(total_len * val_ratio)\n",
        "\n",
        "    # Split the dataset without shuffling\n",
        "    train_data = dataset[:train_len]\n",
        "    val_data = dataset[train_len:train_len+val_len]\n",
        "    test_data = dataset[train_len+val_len:]\n",
        "\n",
        "    return train_data, val_data, test_data"
      ],
      "metadata": {
        "id": "YfZMBs9pGf5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, test_data = split_dataset(x, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)"
      ],
      "metadata": {
        "id": "X8eJTAYtKjyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition and Training"
      ],
      "metadata": {
        "id": "aF3HkxK_iYRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define model\n",
        "\n",
        "input = Input(shape=(None, ))\n",
        "embedding = Embedding(numwords, options.embedding_size, input_length=None)\n",
        "\n",
        "embedded = embedding(input)\n",
        "\n",
        "decoder_lstm = LSTM(options.lstm_capacity, return_sequences=True)\n",
        "h = decoder_lstm(embedded)\n",
        "\n",
        "if options.extra is not None:\n",
        "    for _ in range(options.extra):\n",
        "        h = LSTM(options.lstm_capacity, return_sequences=True)(h)\n",
        "\n",
        "fromhidden = Dense(numwords, activation='linear')\n",
        "out = TimeDistributed(fromhidden)(h)\n",
        "\n",
        "model = Model(input, out)\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=options.lr)\n",
        "lss = sparse_loss\n",
        "\n",
        "model.compile(opt, lss)\n",
        "model.summary()\n",
        "\n",
        "## Training\n",
        "\n",
        "epoch = 0\n",
        "instances_seen = 0\n",
        "while epoch < options.epochs:\n",
        "    for batch in tqdm(x):\n",
        "        n, l = batch.shape\n",
        "\n",
        "        batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)  # prepend start symbol\n",
        "        batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)     # append pad symbol\n",
        "\n",
        "        loss = model.train_on_batch(batch_shifted, batch_out[:, :, None])\n",
        "\n",
        "        instances_seen += n\n",
        "        tbw.add_scalar('lm/batch-loss', float(loss), instances_seen)\n",
        "    print(loss)\n",
        "    epoch += 1\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "KiZROJyBH_0B",
        "outputId": "8d6c15cc-e3ab-4653-811c-9f658f88cce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)           │       \u001b[38;5;34m3,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m570,368\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m2,570,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">570,368</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,140,368\u001b[0m (23.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,140,368</span> (23.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,140,368\u001b[0m (23.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,140,368</span> (23.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:13<00:00, 17.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.47328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:09<00:00, 23.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.128487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 23.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.925706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 23.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.769121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:09<00:00, 23.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.646552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:09<00:00, 24.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.5457444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.4564543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.376974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 21.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.305564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.2391734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:09<00:00, 23.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.1770916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.118429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.0629807\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.0095863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.9584856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.9097686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:09<00:00, 23.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.863028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.817323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.772934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233/233 [00:10<00:00, 22.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.7297025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Models Training"
      ],
      "metadata": {
        "id": "BArVRn6wNfxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(numwords, embedding_size, lstm_capacity, extra_layer=False, reverse=False):\n",
        "    \"\"\"\n",
        "    Create a model with specified parameters\n",
        "\n",
        "    Args:\n",
        "        numwords: vocabulary size\n",
        "        embedding_size: size of word embeddings\n",
        "        lstm_capacity: number of LSTM units\n",
        "        extra_layer: whether to add a second LSTM layer\n",
        "        reverse: whether to process sequence in reverse\n",
        "    \"\"\"\n",
        "    input = Input(shape=(None,))\n",
        "    embedding = Embedding(numwords, embedding_size, input_length=None)\n",
        "    embedded = embedding(input)\n",
        "\n",
        "    # First LSTM layer with go_backwards parameter\n",
        "    h = LSTM(lstm_capacity, return_sequences=True, go_backwards=reverse)(embedded)\n",
        "\n",
        "    # Optional second LSTM layer, also with go_backwards\n",
        "    if extra_layer:\n",
        "        h = LSTM(lstm_capacity, return_sequences=True, go_backwards=reverse)(h)\n",
        "\n",
        "    fromhidden = Dense(numwords, activation='linear')\n",
        "    out = TimeDistributed(fromhidden)(h)\n",
        "\n",
        "    model = Model(input, out)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=options.lr)\n",
        "    model.compile(opt, sparse_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "# The rest of the code remains the same, but now we don't need any sequence reversal in the training function\n",
        "def train_model(model, train_data, val_data, epochs, reverse=False):\n",
        "    \"\"\"\n",
        "    Train the model and return training history\n",
        "    \"\"\"\n",
        "    epoch = 0\n",
        "    instances_seen = 0\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    while epoch < epochs:\n",
        "        epoch_losses = []\n",
        "        print(f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "        for batch in tqdm(train_data):\n",
        "            n, l = batch.shape\n",
        "            batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)\n",
        "            batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)\n",
        "\n",
        "            loss = model.train_on_batch(batch_shifted, batch_out[:, :, None])\n",
        "            epoch_losses.append(loss)\n",
        "\n",
        "            instances_seen += n\n",
        "            tbw.add_scalar('lm/batch-loss', float(loss), instances_seen)\n",
        "\n",
        "        avg_epoch_loss = np.mean(epoch_losses)\n",
        "        training_losses.append(avg_epoch_loss)\n",
        "        print(f'Average training loss: {avg_epoch_loss}')\n",
        "\n",
        "        val_losses = []\n",
        "        for batch in val_data:\n",
        "            n, l = batch.shape\n",
        "            batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)\n",
        "            batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)\n",
        "\n",
        "            val_loss = model.test_on_batch(batch_shifted, batch_out[:, :, None])\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        validation_losses.append(avg_val_loss)\n",
        "        print(f'Validation loss: {avg_val_loss}')\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    return training_losses, validation_losses\n",
        "\n",
        "# Create and train all 4 model configurations\n",
        "models_config = [\n",
        "    {'name': '1layer_forward', 'extra_layer': False, 'reverse': False},\n",
        "    {'name': '1layer_reverse', 'extra_layer': False, 'reverse': True},\n",
        "    {'name': '2layer_forward', 'extra_layer': True, 'reverse': False},\n",
        "    {'name': '2layer_reverse', 'extra_layer': True, 'reverse': True}\n",
        "]\n",
        "\n",
        "# Dictionary to store all models and their training histories\n",
        "results = {}\n",
        "\n",
        "for config in models_config:\n",
        "    print(f\"\\nTraining {config['name']}...\")\n",
        "\n",
        "    model = create_model(\n",
        "        numwords=numwords,\n",
        "        embedding_size=options.embedding_size,\n",
        "        lstm_capacity=options.lstm_capacity,\n",
        "        extra_layer=config['extra_layer'],\n",
        "        reverse=config['reverse']\n",
        "    )\n",
        "\n",
        "    training_losses, validation_losses = train_model(\n",
        "        model=model,\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        epochs=options.epochs,\n",
        "        reverse=config['reverse']\n",
        "    )\n",
        "\n",
        "    results[config['name']] = {\n",
        "        'model': model,\n",
        "        'training_losses': training_losses,\n",
        "        'validation_losses': validation_losses\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GmOcGTPNfHp",
        "outputId": "306717c2-f037-455a-93a5-57680bfa0065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training 1layer_forward...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:09<00:00, 19.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 7.15544319152832\n",
            "Validation loss: 6.525925159454346\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.330569267272949\n",
            "Validation loss: 6.188929080963135\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 28.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.072970390319824\n",
            "Validation loss: 5.982253551483154\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.8986968994140625\n",
            "Validation loss: 5.831508636474609\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.765505790710449\n",
            "Validation loss: 5.71049165725708\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 27.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.655014991760254\n",
            "Validation loss: 5.608805179595947\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.561485290527344\n",
            "Validation loss: 5.521794319152832\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 27.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.478567123413086\n",
            "Validation loss: 5.442885875701904\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.404087543487549\n",
            "Validation loss: 5.372622489929199\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.336971759796143\n",
            "Validation loss: 5.307931423187256\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 27.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.275727272033691\n",
            "Validation loss: 5.249251842498779\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.219201564788818\n",
            "Validation loss: 5.194452285766602\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.165999889373779\n",
            "Validation loss: 5.142965316772461\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.115662574768066\n",
            "Validation loss: 5.093810558319092\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.068131446838379\n",
            "Validation loss: 5.0475945472717285\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.022919178009033\n",
            "Validation loss: 5.003232955932617\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.979538917541504\n",
            "Validation loss: 4.960747241973877\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.938061237335205\n",
            "Validation loss: 4.920105457305908\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.898080825805664\n",
            "Validation loss: 4.880773544311523\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.859298229217529\n",
            "Validation loss: 4.842516899108887\n",
            "\n",
            "Training 1layer_reverse...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:09<00:00, 19.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 7.191488742828369\n",
            "Validation loss: 6.578348636627197\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.4408063888549805\n",
            "Validation loss: 6.364569187164307\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.3010759353637695\n",
            "Validation loss: 6.2568559646606445\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.214776039123535\n",
            "Validation loss: 6.185767650604248\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.15514612197876\n",
            "Validation loss: 6.132585048675537\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.107596397399902\n",
            "Validation loss: 6.0888776779174805\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.064983367919922\n",
            "Validation loss: 6.047408580780029\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.0243024826049805\n",
            "Validation loss: 6.008358001708984\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.987215518951416\n",
            "Validation loss: 5.97176456451416\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 26.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.94997501373291\n",
            "Validation loss: 5.934841632843018\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.913032054901123\n",
            "Validation loss: 5.897683620452881\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.875569820404053\n",
            "Validation loss: 5.85999870300293\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.838181972503662\n",
            "Validation loss: 5.822775363922119\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.8015851974487305\n",
            "Validation loss: 5.7862324714660645\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.765075206756592\n",
            "Validation loss: 5.74969482421875\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.728250980377197\n",
            "Validation loss: 5.712698936462402\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.6912407875061035\n",
            "Validation loss: 5.675642013549805\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 25.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.654431343078613\n",
            "Validation loss: 5.638924598693848\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.618058204650879\n",
            "Validation loss: 5.6028361320495605\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:06<00:00, 26.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.582178115844727\n",
            "Validation loss: 5.56758451461792\n",
            "\n",
            "Training 2layer_forward...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:11<00:00, 16.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 7.157709121704102\n",
            "Validation loss: 6.653712272644043\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 23.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.5732316970825195\n",
            "Validation loss: 6.526325225830078\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 21.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.481379508972168\n",
            "Validation loss: 6.4409708976745605\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.399699687957764\n",
            "Validation loss: 6.3615264892578125\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 23.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.328004360198975\n",
            "Validation loss: 6.29648494720459\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.264010429382324\n",
            "Validation loss: 6.235597133636475\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.2083740234375\n",
            "Validation loss: 6.184521675109863\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.160441875457764\n",
            "Validation loss: 6.139675140380859\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.11814546585083\n",
            "Validation loss: 6.099665641784668\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.08072566986084\n",
            "Validation loss: 6.063830375671387\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.0482611656188965\n",
            "Validation loss: 6.033127307891846\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.01742696762085\n",
            "Validation loss: 6.002236366271973\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.985553741455078\n",
            "Validation loss: 5.969547271728516\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.952202796936035\n",
            "Validation loss: 5.935629367828369\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.918373107910156\n",
            "Validation loss: 5.901828289031982\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.8831868171691895\n",
            "Validation loss: 5.865654945373535\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 24.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.846371650695801\n",
            "Validation loss: 5.828162670135498\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.808647632598877\n",
            "Validation loss: 5.790493011474609\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.770757675170898\n",
            "Validation loss: 5.752716064453125\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.733321189880371\n",
            "Validation loss: 5.715590000152588\n",
            "\n",
            "Training 2layer_reverse...\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:11<00:00, 16.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 7.070652484893799\n",
            "Validation loss: 6.542620658874512\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.420408725738525\n",
            "Validation loss: 6.333899021148682\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 6.20885705947876\n",
            "Validation loss: 6.070540904998779\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 23.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.894490718841553\n",
            "Validation loss: 5.699398994445801\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.5012359619140625\n",
            "Validation loss: 5.308444023132324\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 5.130908966064453\n",
            "Validation loss: 4.9624176025390625\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.809423446655273\n",
            "Validation loss: 4.663608074188232\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.529208183288574\n",
            "Validation loss: 4.399486541748047\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.2789764404296875\n",
            "Validation loss: 4.1624627113342285\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 4.053985118865967\n",
            "Validation loss: 3.9490983486175537\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.8511672019958496\n",
            "Validation loss: 3.756387710571289\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.667461395263672\n",
            "Validation loss: 3.5811896324157715\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.499885082244873\n",
            "Validation loss: 3.420886754989624\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.3461859226226807\n",
            "Validation loss: 3.2736010551452637\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.2047834396362305\n",
            "Validation loss: 3.1379120349884033\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 3.074256181716919\n",
            "Validation loss: 3.0123283863067627\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 21.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 2.953261613845825\n",
            "Validation loss: 2.8958513736724854\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:07<00:00, 23.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 2.840996265411377\n",
            "Validation loss: 2.7876527309417725\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 2.73661732673645\n",
            "Validation loss: 2.6870157718658447\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186/186 [00:08<00:00, 22.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 2.6393094062805176\n",
            "Validation loss: 2.5928659439086914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tasks 2,5,6,8,9"
      ],
      "metadata": {
        "id": "x_-Va6rahrUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_probability(model, sentence, w2i):\n",
        "    \"\"\"\n",
        "    Calculate the probability of a sentence according to the model.\n",
        "\n",
        "    Args:\n",
        "        model: trained Keras model\n",
        "        sentence: string of words\n",
        "        w2i: word to index dictionary\n",
        "\n",
        "    Returns:\n",
        "        probability of the sentence\n",
        "    \"\"\"\n",
        "    # Convert sentence to indices\n",
        "    words = sentence.lower().split()\n",
        "    indices = [w2i.get(word, w2i['<UNK>']) for word in words]\n",
        "\n",
        "    # Prepare input sequence\n",
        "    input_seq = np.array([1] + indices[:-1])[None, :]  # Add start token and batch dimension\n",
        "    target_seq = np.array(indices)[None, :, None]  # Add batch and feature dimensions\n",
        "\n",
        "    # Get model predictions\n",
        "    predictions = model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Calculate probability as product of individual word probabilities\n",
        "    log_prob = 0\n",
        "    for i, word_idx in enumerate(indices):\n",
        "        # Get softmax probabilities for current position\n",
        "        logits = predictions[0, i]\n",
        "        probs = np.exp(logits - np.max(logits))\n",
        "        probs = probs / np.sum(probs)\n",
        "\n",
        "        # Add log probability of current word\n",
        "        log_prob += np.log(probs[word_idx] + 1e-10)\n",
        "\n",
        "    return np.exp(log_prob)\n",
        "\n",
        "def deconstruct_sentence(model, start_words, length, temperature, w2i, i2w):\n",
        "    \"\"\"\n",
        "    Generate sentences backwards from given start words at different temperatures.\n",
        "\n",
        "    Args:\n",
        "        model: trained Keras model\n",
        "        start_words: string of starting words\n",
        "        length: desired total length of sentence\n",
        "        temperature: sampling temperature\n",
        "        w2i: word to index dictionary\n",
        "        i2w: index to word dictionary\n",
        "\n",
        "    Returns:\n",
        "        generated sentence\n",
        "    \"\"\"\n",
        "    words = start_words.lower().split()\n",
        "    current_length = len(words)\n",
        "\n",
        "    # Convert initial words to indices\n",
        "    indices = [w2i.get(word, w2i['<UNK>']) for word in words]\n",
        "\n",
        "    while current_length < length:\n",
        "        # Prepare input sequence\n",
        "        input_seq = np.array([1] + indices)[None, :]  # Add start token and batch dimension\n",
        "\n",
        "        # Get model predictions\n",
        "        predictions = model.predict(input_seq, verbose=0)\n",
        "        next_word_logits = predictions[0, len(indices)]\n",
        "\n",
        "        # Apply temperature\n",
        "        if temperature == 0:\n",
        "            next_word_idx = np.argmax(next_word_logits)\n",
        "        else:\n",
        "            scaled_logits = next_word_logits / temperature\n",
        "            probs = np.exp(scaled_logits - np.max(scaled_logits))\n",
        "            probs = probs / np.sum(probs)\n",
        "            next_word_idx = np.random.choice(len(probs), p=probs)\n",
        "\n",
        "        indices.append(next_word_idx)\n",
        "        words.append(i2w[next_word_idx])\n",
        "        current_length += 1\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Calculate perplexity for all models on all datasets\n",
        "def calculate_perplexity(model, data):\n",
        "    \"\"\"Calculate perplexity for a model on given data.\"\"\"\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in data:\n",
        "        n, l = batch.shape\n",
        "        batch_shifted = np.concatenate([np.ones((n, 1)), batch], axis=1)\n",
        "        batch_out = np.concatenate([batch, np.zeros((n, 1))], axis=1)\n",
        "\n",
        "        loss = model.test_on_batch(batch_shifted, batch_out[:, :, None])\n",
        "        total_loss += loss * n\n",
        "        total_samples += n\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    return np.exp(avg_loss)\n",
        "\n",
        "# Now let's run all the analyses\n",
        "\n",
        "# 1. Calculate perplexity for all models on all datasets\n",
        "print(\"\\nPerplexity Results:\")\n",
        "print(\"Model\\t\\tTrain\\tValidation\\tTest\")\n",
        "for config in models_config:\n",
        "    name = config['name']\n",
        "    model = results[name]['model']\n",
        "\n",
        "    train_perp = calculate_perplexity(model, train_data)\n",
        "    val_perp = calculate_perplexity(model, val_data)\n",
        "    test_perp = calculate_perplexity(model, test_data)\n",
        "\n",
        "    print(f\"{name}\\t{train_perp:.2f}\\t{val_perp:.2f}\\t{test_perp:.2f}\")\n",
        "\n",
        "# 2. Generate sentences at different temperatures\n",
        "print(\"\\nSentence Generation at Different Temperatures:\")\n",
        "start_words = \"love I\"\n",
        "for temp in [0.1, 1.0, 10.0]:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    for config in models_config:\n",
        "        name = config['name']\n",
        "        model = results[name]['model']\n",
        "        sentence = deconstruct_sentence(model, start_words, 7, temp, w2i, i2w)\n",
        "        print(f\"{name}: {sentence}\")\n",
        "\n",
        "# 3. Calculate probabilities for generated sentences and \"love i cupcakes\"\n",
        "print(\"\\nSentence Probabilities:\")\n",
        "sentences_to_check = []\n",
        "# Add generated sentences\n",
        "for temp in [0.1, 1.0, 10.0]:\n",
        "    for config in models_config:\n",
        "        name = config['name']\n",
        "        model = results[name]['model']\n",
        "        sentence = deconstruct_sentence(model, start_words, 7, temp, w2i, i2w)\n",
        "        sentences_to_check.append(sentence)\n",
        "# Add additional sentence\n",
        "sentences_to_check.append(\"love i cupcakes\")\n",
        "\n",
        "for sentence in sentences_to_check:\n",
        "    print(f\"\\nProbabilities for: {sentence}\")\n",
        "    for config in models_config:\n",
        "        name = config['name']\n",
        "        model = results[name]['model']\n",
        "        prob = calculate_sentence_probability(model, sentence, w2i)\n",
        "        print(f\"{name}: {prob:.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7UMXLqqXT_J",
        "outputId": "8ce233f6-b831-4df9-84e8-cfbb7be4eb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity Results:\n",
            "Model\t\tTrain\tValidation\tTest\n",
            "1layer_forward\t105.69\t104.90\t105.20\n",
            "1layer_reverse\t231.39\t229.91\t230.92\n",
            "2layer_forward\t255.51\t253.78\t253.95\n",
            "2layer_reverse\t7.15\t7.01\t6.99\n",
            "\n",
            "Sentence Generation at Different Temperatures:\n",
            "\n",
            "Temperature: 0.1\n",
            "1layer_forward: love i of the <UNK> lrb <UNK>\n",
            "1layer_reverse: love i <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "2layer_forward: love i is a <UNK> of the\n",
            "2layer_reverse: love i <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "\n",
            "Temperature: 1.0\n",
            "1layer_forward: love i <UNK> leaves '' <UNK> weapon\n",
            "1layer_reverse: love i <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "2layer_forward: love i family latd marrow or al\n",
            "2layer_reverse: love i days pp oil philosopher khan\n",
            "\n",
            "Temperature: 10.0\n",
            "1layer_forward: love i 1992 physicians joel lawyer incorrectly\n",
            "1layer_reverse: love i million it inverness remaining louis\n",
            "2layer_forward: love i accounts zack zeta manipulation intelligence\n",
            "2layer_reverse: love i country italy teaching basilica tenor\n",
            "\n",
            "Sentence Probabilities:\n",
            "\n",
            "Probabilities for: love i of the <UNK> lrb <UNK>\n",
            "1layer_forward: 3.59e-11\n",
            "1layer_reverse: 5.68e-15\n",
            "2layer_forward: 2.68e-12\n",
            "2layer_reverse: 3.45e-05\n",
            "\n",
            "Probabilities for: love i <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "1layer_forward: 1.38e-10\n",
            "1layer_reverse: 5.82e-13\n",
            "2layer_forward: 1.73e-11\n",
            "2layer_reverse: 4.35e-03\n",
            "\n",
            "Probabilities for: love i is a <UNK> <UNK> <UNK>\n",
            "1layer_forward: 4.74e-13\n",
            "1layer_reverse: 4.99e-14\n",
            "2layer_forward: 4.00e-12\n",
            "2layer_reverse: 2.09e-03\n",
            "\n",
            "Probabilities for: love i on pas <PAD> <PAD> <PAD>\n",
            "1layer_forward: 2.84e-21\n",
            "1layer_reverse: 2.28e-19\n",
            "2layer_forward: 1.01e-17\n",
            "2layer_reverse: 9.22e-04\n",
            "\n",
            "Probabilities for: love i 2 <UNK> <UNK> 117 have\n",
            "1layer_forward: 1.09e-18\n",
            "1layer_reverse: 1.93e-25\n",
            "2layer_forward: 1.84e-21\n",
            "2layer_reverse: 6.24e-17\n",
            "\n",
            "Probabilities for: love i <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "1layer_forward: 1.38e-10\n",
            "1layer_reverse: 5.82e-13\n",
            "2layer_forward: 1.73e-11\n",
            "2layer_reverse: 4.35e-03\n",
            "\n",
            "Probabilities for: love i is usually known in some\n",
            "1layer_forward: 1.38e-17\n",
            "1layer_reverse: 2.17e-17\n",
            "2layer_forward: 4.66e-15\n",
            "2layer_reverse: 1.71e-08\n",
            "\n",
            "Probabilities for: love i insurance soccer toledo bamboo either\n",
            "1layer_forward: 2.23e-37\n",
            "1layer_reverse: 1.22e-38\n",
            "2layer_forward: 2.37e-30\n",
            "2layer_reverse: 1.80e-28\n",
            "\n",
            "Probabilities for: love i wire cincinnati wii repositories thirty\n",
            "1layer_forward: 4.00e-35\n",
            "1layer_reverse: 7.57e-30\n",
            "2layer_forward: 1.61e-31\n",
            "2layer_reverse: 7.86e-29\n",
            "\n",
            "Probabilities for: love i ecozones ea silent remove gallery\n",
            "1layer_forward: 7.98e-42\n",
            "1layer_reverse: 3.14e-36\n",
            "2layer_forward: 3.86e-33\n",
            "2layer_reverse: 3.13e-29\n",
            "\n",
            "Probabilities for: love i mb 2008 ibn returns ivan\n",
            "1layer_forward: 5.44e-35\n",
            "1layer_reverse: 4.96e-33\n",
            "2layer_forward: 1.33e-31\n",
            "2layer_reverse: 3.08e-28\n",
            "\n",
            "Probabilities for: love i boxes entries heidelbergensis distance capitol\n",
            "1layer_forward: 1.26e-35\n",
            "1layer_reverse: 7.82e-31\n",
            "2layer_forward: 6.24e-30\n",
            "2layer_reverse: 5.31e-27\n",
            "\n",
            "Probabilities for: love i cupcakes\n",
            "1layer_forward: 2.27e-08\n",
            "1layer_reverse: 1.35e-10\n",
            "2layer_forward: 7.95e-09\n",
            "2layer_reverse: 3.70e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Predict next word"
      ],
      "metadata": {
        "id": "fHq_MWXAhlJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_word_prediction(model, w2i, i2w, word):\n",
        "    \"\"\"\n",
        "    Given a trained model, vocabulary mappings (w2i, i2w), and an input word,\n",
        "    predict the next word and return the prediction with probabilities.\n",
        "    \"\"\"\n",
        "    word = word.lower().strip()\n",
        "\n",
        "    if word not in w2i:\n",
        "        return f\"Word '{word}' not in vocabulary. Please try another word.\"\n",
        "\n",
        "    word_idx = w2i[word]\n",
        "    input_seq = np.array([[1, word_idx]])  # Add batch dimension\n",
        "\n",
        "    try:\n",
        "        # Use model.predict to get predictions\n",
        "        predictions = model.predict(input_seq, verbose=0)\n",
        "        next_word_logits = predictions[0, 1]\n",
        "\n",
        "        # Apply temperature to logits\n",
        "        exp_logits = np.exp(next_word_logits - np.max(next_word_logits))\n",
        "        probs = exp_logits / np.sum(exp_logits)\n",
        "\n",
        "        # Get top 5 predictions\n",
        "        top_indices = np.argsort(probs)[-5:][::-1]\n",
        "\n",
        "        result = \"\\nTop 5 predicted next words:\\n\"\n",
        "        for idx in top_indices:\n",
        "            result += f\"{i2w[idx]}: {probs[idx]:.3f}\\n\"\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"Error during prediction: {str(e)}\"\n",
        "\n",
        "# Example of how to call it manually:\n",
        "# Replace these with your actual word-to-index and index-to-word dictionaries and model\n",
        "word = \"love\"\n",
        "model = results['2layer_reverse']['model']  # Use the model you want\n",
        "output = get_next_word_prediction(model, w2i, i2w, word)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJTqq9rAecVp",
        "outputId": "0819ec05-fb68-481e-97a5-91ddfde44267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 predicted next words:\n",
            "core: 0.024\n",
            "basketball: 0.017\n",
            "priest: 0.012\n",
            "distance: 0.012\n",
            "events: 0.011\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
